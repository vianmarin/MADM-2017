{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "<!-- requirement: img/Data_Frame_Data_Series.png -->\n",
    "<!-- requirement: small_data/fha_by_tract.csv -->\n",
    "<!-- requirement: small_data/2013_Gaz_tracts_national.tsv -->\n",
    "\n",
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e. when everything fits into memory.\n",
    "\n",
    "The basic new \"noun\" in pandas is the **data frame**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns (objects) in Pandas\n",
    "\n",
    "### Data Frames\n",
    "\n",
    "Like a table, with rows and columns (e.g. as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. Like in SQL, columns are type homogeneous.\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "### Data Series:\n",
    "\n",
    "These are named columns of a DataFrame (more correctly, a dataframe is a dictionary of Series).  The entries of the series have homogenous type.\n",
    "\n",
    "<img src=\"img/4-pandas/Data_Frame_Data_Series.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# a data frame\n",
    "df1 = pd.DataFrame({\n",
    "    'number': [1, 2, 3],\n",
    "    'animal': ['cat', 'dog', 'mouse']\n",
    "})\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select the animal series from the dataframe using the different notations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the types of the dataframe columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same data frame\n",
    "df2 = pd.DataFrame([\n",
    "    ('cat', 1),\n",
    "    ('dog', 2),\n",
    "    ('mouse', 3),\n",
    "], columns=['animal', 'number'])\n",
    "\n",
    "np.all(df1 == df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbs (operations) in Pandas\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e. SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from another column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation:** If you're an Excel cognoscenti, you may appreciate this.\n",
    "  - **NA handling:** Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "We'll go through a little on all of these in the context of an example.\n",
    "\n",
    "We're going to explore a dataset of mortgage insurance issued by the Federal Housing Authority (FHA).  The data is broken down by census tract and tells us how big of a player the FHA is in each tract (how many homes etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data (and basic statistics / visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\",\n",
    "        \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\",\n",
    "        \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "\n",
    "df = pd.read_csv('data/fha_by_tract.csv', names=names)  # Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create a new column from a combination of the others 'Census_Tract_Number', 'County_Code'] and 'State_Code'\n",
    "df['GEOID'] = df['Census_Tract_Number']*100 + 10**6 * df['County_Code'] \\\n",
    "    + 10**9 * df['State_Code']   # A computed field!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_to_drop = # the column name you created before\n",
    "df.drop(column_to_drop, axis = 1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most operations produce copies (unless `inplace=True` is specified).  The `df` object still has the GEOID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_to_drop in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To use inplace=True is not advised. It is better to assign the transformed dataframe to a new variable\n",
    "\n",
    "df_new = # drop the created column\n",
    "\n",
    "print(column_to_drop in df.columns)\n",
    "\n",
    "print(column_to_drop in df_new.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows can also be dropped.  Note that the indices do not reset.  The index is associated with the row, not with the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(0, axis=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, rows are indexed by their position.  However, any column can be made into an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index('State_Code').head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple levels of indexing is possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.set_index(['State_Code', 'County_Code']).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An index can be turned back into a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index('State_Code').reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of mortages in each census tract insured by FHA\")\n",
    "df['PCT_AMT_FHA'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply describe to the entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot an histogram of the column PCT_AMT_FHA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above distribution looks skewed, so let's look at its logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LOG_AMT_ALL'] = np.log1p(df['AMT_ALL'])  # Create a new column to examine\n",
    "print(df['LOG_AMT_ALL'].describe())\n",
    "\n",
    "df['AMT_ALL'].apply(np.log1p).hist(bins=500)  # Or apply a function to each element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing data frames\n",
    "\n",
    "Indexing by a column name yields a data series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['State_Code'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing by a list of column names gives another data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['State_Code', 'County_Code']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What will this return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(df[['State_Code']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['State_Code']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data frame is an iterator that yields the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[col for col in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select specific rows, you can try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To index a particular element of the frame, use the `.loc` attribute.  It takes index and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3, 'State_Code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both can be sliced.  Unusually for Python, both endpoints are included in the slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:3, 'State_Code':'Census_Tract_Number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position-based indexing is available in the `.iloc` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual slicing convention is used for `.iloc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:3, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data\n",
    "\n",
    "Now the `df[...]` notation is very flexible:\n",
    "  - It accepts column names (strings and lists of strings);\n",
    "  - It accepts column numbers (so long as there is no ambiguity with column names);\n",
    "  - It accepts _binary data series!_\n",
    "  \n",
    "This means that you can write\n",
    "```python\n",
    "\n",
    " df[ df['column_name2'] == 'MD' & ( df['column_name1']==5 | df['column_name1']==6 ) ]\n",
    "```   \n",
    "for what you would write in SQL as\n",
    "\n",
    "```sql\n",
    "SELECT * FROM df\n",
    "WHERE column_name2=\"MD\" AND (column_name1=5 OR column_name1=6)\n",
    "```           \n",
    "Boolean operators on a data frame return a data series of bools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(df['State_Code'] == 1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be combined with the (bitwise) boolean operators.  Note that, due to operator precedence, you want to wrap the individual comparisons in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((df['State_Code'] == 1) & (df['Census_Tract_Number'] == 9613)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frames accept indexing by boolean series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df['State_Code'] == 5].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** select rows by binary data series only if they share the same data index!\n",
    "\n",
    "**Exercise:**\n",
    "1. Plot the histogram of percentages for different states in the same graph to compare them.\n",
    "2. Notice that there is a spike at 100%.  This means that the FHA has insured 100% of the houses in that census tract.  See what happens to the histogram when we restrict it to the case where the total number of loans is non-negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining data\n",
    "\n",
    "The analogue of a\n",
    "\n",
    ">             \n",
    "    SELECT * \n",
    "        FROM df1\n",
    "        INNER JOIN df2 \n",
    "        ON df1.field_name=df2.field_name;\n",
    "\n",
    "is\n",
    "\n",
    "    df_joined = df1.merge(df2, on='field_name')\n",
    "\n",
    "You can also do left / right / outer joins, mix-and-match column names, etc.  For that consult the Pandas documentation. (The example below will do a left join.)\n",
    "\n",
    "Of course, just looking at the distribution of insurance by census tract isn't interesting unless we know more about the census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first row is the column names, so we don't have to specify those\n",
    "df_geo = pd.read_csv('data/2013_Gaz_tracts_national.tsv', sep='\\t')\n",
    "df_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df.merge(df_geo, on='GEOID', how='left')\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data\n",
    "\n",
    "The analog of SQL's `GROUP BY` is\n",
    "\n",
    "    grouped = df.groupby(['field_name1', ...])...\n",
    "\n",
    "The above is analogous to\n",
    ">             \n",
    "    SELECT mean(df.value1), std(df.value2) \n",
    "        FROM df\n",
    "        GROUP BY df.field_name1, ...\n",
    "\n",
    "Pandas is somewhat more flexible in how you can use grouping, not requiring you to specify an aggregation function up front.  The `.groupby()` method that can later be aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usps_groups = df_joined.groupby('USPS')\n",
    "print(usps_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason Pandas doesn't require you to specify an aggregation function up front is because the groupby method by itself does little work. It returns a `DataFrameGroupBy` datatype that contains a dictionary of group keys to lists of row numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(usps_groups.groups))\n",
    "usps_groups.groups['AK'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usps_groups.groups.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can retrieve the group of data associated with one key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usps_groups.get_group('AK')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that this is the same as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.iloc[usps_groups.groups['AK'][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usps_groups.mean().head()  # Takes the mean of the rows in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the analog of\n",
    "# SELECT USPS, SUM(AMT_FHA), SUM(AMT_ALL), ... FROM df GROUP BY USPS;\n",
    "df_by_state = usps_groups['AMT_FHA', 'AMT_ALL', 'NUM_FHA', 'NUM_ALL'].sum()\n",
    "df_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_state['PCT_AMT_FHA'] = 100.0 * df_by_state['AMT_FHA']  / df_by_state['AMT_ALL']\n",
    "\n",
    "# This sure looks different than the census-tract level histogram!\n",
    "df_by_state['PCT_AMT_FHA'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify a specific aggregation function per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usps_groups['NUM_FHA', 'NUM_ALL'].agg({'NUM_FHA': np.sum, 'NUM_ALL': np.mean}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groupby function is especially useful when you define your own aggregation functions. Here, we define a function that returns the row for the census track located farthest to the north. The apply function attempts to 'combine results together in an intelligent way.' The list of Series objects from each call to `farthest_north` for each USPS code is collapsed into a single DataFrame table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def farthest_north(state_df):\n",
    "    # descending sort, then select row 0\n",
    "    # the datatype will be a pandas Series\n",
    "    return state_df.sort_values('INTPTLAT', ascending=False).iloc[0]\n",
    "\n",
    "df_joined.groupby('USPS').apply(farthest_north)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting by indices and columns\n",
    "\n",
    "We can sort by the row (or column) index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_state.sort_index(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort by the value in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_state.sort_values('AMT_FHA').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique values\n",
    "\n",
    "As in SQL, pandas can compute unique values, value counts, and test for membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['State_Code'].unique()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['State_Code'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['State_Code'].isin(df['State_Code'].head(3)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing and NA data\n",
    "\n",
    "\n",
    "When you read in a CSV file / SQL database there are often \"NA\" (or \"null\", \"None\", etc.) values.  The CSV reader has a special field for specifying how this is denoted, and SQL has the built-in notion of NULL.  Pandas provides some tools for working with these -- they are generally similar to (and a little bit worse than) `R`.\n",
    "\n",
    "Note that these methods are by default not in place -- that is, they create a new series and do not change the original one.\n",
    "\n",
    "For more details: http://pandas.pydata.org/pandas-docs/stable/missing_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GEOID'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.isnull()` and `.notnull()` test for null-ness and return a Boolean series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GEOID'].isnull()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.dropna()` removes the rows with null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GEOID'].size, df['GEOID'].dropna().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.fillna()` replaces N/A values with another value.  `.interpolate()` replaces null values by (linear, or quadratic, or...) interpolation.  There is support for indexing by times (not necessarily equally spaced), etc. in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fill_0'] = df['GEOID'].fillna(0)                          # Fills constant value, here 0\n",
    "df['fill_forward'] = df['GEOID'].fillna(method='ffill')       # Fill forwards\n",
    "df['fill_back'] = df['GEOID'].fillna(method='bfill', limit=5) # Fill backwards, at most 5\n",
    "df['fill_mean'] = df['GEOID'].fillna(df['GEOID'].mean())      # Fills constant value, here the mean (imputation)\n",
    "df['fill_interp'] = df['GEOID'].interpolate()                 # Fills interpolated value\n",
    "df[['GEOID', 'fill_0', 'fill_forward', 'fill_back', 'fill_mean', 'fill_interp']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "N/A values are (usually) smartly ignored when performing other calculations on dataframes. For example, when using string methods on series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_series = df['GEOID'].replace(0, np.nan).apply(str)\n",
    "print(text_series[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_series[:10].str.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying mean on numeric data ignores NA's by default (check docs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GEOID'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating strings\n",
    "\n",
    "Element-wise string operations are available through the `.str` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = df_joined['USPS'].dropna()\n",
    "states[states.str.contains('A')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices in Pandas\n",
    "\n",
    "Pandas indices allow us to handle data naturally.  **Elements are associated based on their index, not their order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([1,2,3], index=['a', 'b', 'c'])\n",
    "s2 = pd.Series([3,2,1], index=['c', 'b', 'a'])\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = pd.Series([3,2,1], index=['c', 'd', 'e'])\n",
    "s1 + s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values get a NaN, but this can be replaced by a fill value of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.add(s3, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function application and mapping\n",
    "\n",
    "For element-wise function application, the most straightforward thing to do is to apply numpy functions to these objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(24).reshape(4,6))\n",
    "\n",
    "np.sin(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This relies on numpy functions automatically broadcasting themselves to work element-wise.  To apply a pure-python function to each element, use the `.applymap()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.applymap(lambda x: \"%.2f\" % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, sometimes you want to compute things column-wise or row-wise.  In this case, you will need to use the `apply` method. \n",
    "\n",
    "For example, the following takes the range of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.apply(lambda x: x.max() - x.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this takes the range of reach row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.apply(lambda x: x.max() - x.min(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas HTML data import example\n",
    "\n",
    "Pandas takes a \"batteries included\" approach and throws in a whole lot of convenience functions.  For instance it has import functions for a variety of formats.  One of the pleasant surprises is a command `read_html` that's meant to automate the process of extracting tabular data from HTML.  In particular, it works pretty well with tables on Wikipedia.  \n",
    "\n",
    "Let's do an example: We'll try to extract the list of the world's tallest structures from\n",
    "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_html('http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world', header=0, parse_dates=False)\n",
    "\n",
    "# There are several tables on the page.  By inspection we can figure out which one we want\n",
    "tallest = dfs[3]\n",
    "print(tallest.columns)\n",
    "# The coordinates column needs to be fixed up.  This is a bit of string parsing:\n",
    "def clean_lat_long(s):\n",
    "    try:\n",
    "        parts = s.split(\"/\")\n",
    "    except AttributeError:\n",
    "        return (None, None)\n",
    "    if len(parts) < 3:\n",
    "        return (None, None)\n",
    "    m = re.search(r\"(\\d+[.]\\d+);[^\\d]*(\\d+[.]\\d+)[^\\d]\", parts[2])\n",
    "    if not m:\n",
    "        return (None, None)\n",
    "    return (m.group(1), m.group(2))\n",
    "\n",
    "tallest['Clean_Coordinates'] = tallest['Coordinates'].apply(clean_lat_long)\n",
    "tallest['Latitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[0])\n",
    "tallest['Longitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[1])\n",
    "\n",
    "# Et voila\n",
    "tallest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Parse the table rankings of [UK universities available on Wikipedia](https://en.wikipedia.org/wiki/Rankings_of_universities_in_the_United_Kingdom):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Timestamps\n",
    "\n",
    "Pandas comes with excellent tools for managing temporal data. Central to this is the Timestamp class, which can infer timestamps from many sensible inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Timestamp('July 4, 2016'))\n",
    "print(pd.Timestamp('Monday, July 4, 2016'))\n",
    "print(pd.Timestamp('Tuesday, July 4th, 2016'))  # notice it ignored 'Tuesday'\n",
    "print(pd.Timestamp('Monday, July 4th, 2016 05:00 PM'))\n",
    "print(pd.Timestamp('04/07/2016T17:20:13.123456'))\n",
    "print(pd.Timestamp(1467651600000000000))  # number of ns since the epoch, 1/1/1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can manage timestamps with timezones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "july4 = pd.Timestamp('Monday, July 4th, 2016 05:00 PM').tz_localize('US/Eastern')\n",
    "labor_day = pd.Timestamp('9/5/2016 12:00', tz='US/Eastern')\n",
    "thanksgiving = pd.Timestamp('11/24/2016 16:00')  # no timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can do calculations on Timestamps if they are both localized to the same timezone or neither has a timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labor_day - july4)\n",
    "# print thanksgiving - july4  # generates an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series offsets are useful for calculating dates relative to another date. Observe it skips over weekend days but is oblivious to holidays. Pandas does support [Custom Calendars](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#holidays-holiday-calendars) if you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import BDay, Day, BMonthEnd\n",
    "\n",
    "print(july4 + Day(5))  # 5 calendar days later, a Saturday.\n",
    "print(july4 + BDay(5))  # 5 business days later, or the following Monday.\n",
    "print(july4 - BDay(1))  # 1 business day earlier, or the previous Friday.\n",
    "print(july4 + BMonthEnd(1))  # last business day of the month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can generate a range of dates. Here, we generate a list of business days in January of 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_days = pd.date_range('1/1/2016', '1/31/2016', freq='B')\n",
    "business_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can in turn be used in as a DataFrame index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame(np.random.rand(len(business_days)),\n",
    "                    index=business_days,\n",
    "                   columns=['random'])\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same timezone functions are still available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.tz_localize('UTC').tz_convert('US/Pacific').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-indices, stacking, and pivot tables\n",
    "\n",
    "Data frames can contain multiple indices for rows or columns.  For example, grouping by two columns will produce a two-level row index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['State_Code', 'County_Code'])[['NUM_ALL', 'NUM_FHA']].sum()\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A row index can be converted into a column index with the `.unstack()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped.unstack().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `.stack()` method does the opposite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(grouped.unstack().stack() == grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done with one step with the `pivot_table()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df, index='State_Code', columns='County_Code',\n",
    "               values=['NUM_ALL', 'NUM_FHA'], aggfunc=np.sum).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may already by familiar with pivot tables in Excel.  These work similarly, and area  good tool for changing the dependent and independent variables for aggregations of data. See http://pandas.pydata.org/pandas-docs/stable/reshaping.html for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plugging into more advanced analytics\n",
    "\n",
    "Almost any \"advanced analytics\" tool in the Python ecosystem is going to take as input `np.array` type arrays.  You can access the underlying array of a data frame column as\n",
    "\n",
    "        df['column'].values\n",
    "        \n",
    "Many of them take `nd.array` whose underlying data can be accessed by \n",
    "\n",
    "        df.values\n",
    "        \n",
    "directly.  *Most* of the time, they will take `df['column']` and `df` without needing to look at values.\n",
    "\n",
    "This is particularly important if you want to use Pandas with the sklearn library. See this [blog post](http://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/) for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. Describe several situations where you would prefer to use Pandas, and several where you'd prefer SQL.\n",
    "1. Write the syntax for slicing the rows of a dataframe according to a filtering criterion.\n",
    "1. Write the syntax for selecting, adding, or removing columns of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This will load the iris dataset.\n",
    "### Perfom a Exploratory Data Analysis using pandas\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "\n",
    "print(iris_data.DESCR)\n",
    "\n",
    "df= pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "df['label'] = iris_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
